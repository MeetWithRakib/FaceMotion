<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>FaceMotion</title>

<link href="https://fonts.googleapis.com/css2?family=Share+Tech+Mono&display=swap" rel="stylesheet">

<style>
:root{
  --neon:#00ffcc;
  --bg:#020202;
  --glass:rgba(0,255,204,0.08);
}

*{box-sizing:border-box}

body{
  margin:0;
  background:radial-gradient(circle at top,#050505,#000);
  color:var(--neon);
  font-family:'Share Tech Mono',monospace;
  display:flex;
  justify-content:center;
  min-height:100vh;
  overflow-x: hidden;
}

/* Modal Styling */
#modalOverlay {
  position: fixed;
  top: 0; left: 0; width: 100%; height: 100%;
  background: rgba(0,0,0,0.9);
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 1000;
}

.modal-content {
  background: #111;
  padding: 40px;
  border: 1px solid var(--neon);
  border-radius: 15px;
  text-align: center;
  box-shadow: 0 0 50px rgba(0,255,204,0.2);
}

.container{
  width:100%;
  max-width:900px;
  padding:20px;
  opacity: 0.3; /* Hidden until models load */
  pointer-events: none;
  transition: 0.5s;
}

.container.active {
  opacity: 1;
  pointer-events: auto;
}

h1{
  text-align:center;
  text-shadow:0 0 20px var(--neon);
  letter-spacing: 2px;
}

.panel{
  background:var(--glass);
  backdrop-filter:blur(10px);
  border:1px solid var(--neon);
  border-radius:14px;
  padding:16px;
  margin-bottom:15px;
  box-shadow:0 0 30px rgba(0,255,204,.15);
}

.controls{
  display:flex;
  flex-wrap:wrap;
  gap:12px;
  justify-content:center;
}

button{
  background:transparent;
  color:var(--neon);
  border:1px solid var(--neon);
  padding:12px 25px;
  border-radius:8px;
  font-family: inherit;
  font-weight: bold;
  cursor:pointer;
  transition:.3s;
}

button:hover:not(:disabled){
  background:rgba(0,255,204,.15);
  box-shadow:0 0 20px var(--neon);
  transform: translateY(-2px);
}

button:disabled{
  opacity:.3;
  border-color: #444;
  color: #444;
  cursor:not-allowed;
}

.file-btn{
  position:relative;
  display: inline-block;
}

.file-btn input{
  position:absolute;
  inset:0;
  opacity:0;
  cursor:pointer;
}

canvas{
  width:100%;
  height: auto;
  border-radius:12px;
  border:1px solid var(--neon);
  background: #000;
}

#status{
  white-space:pre-line;
  line-height:1.6;
  min-height: 50px;
}

.loading-spinner {
  border: 4px solid rgba(0,255,204,0.1);
  border-left: 4px solid var(--neon);
  border-radius: 50%;
  width: 40px;
  height: 40px;
  animation: spin 1s linear infinite;
  margin: 20px auto;
  display: none;
}

@keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }

.footer{
  text-align:center;
  font-size:12px;
  opacity:.7;
  margin-top: 20px;
}
</style>
</head>

<body>

<div id="modalOverlay">
  <div class="modal-content">
    <h2>FaceMotion</h2>
    <p>AI Models Are Required For Facial Recognition.</p>
    <div id="spinner" class="loading-spinner"></div>
    <button id="loadModelsBtn">INITIALIZE AI MODELS</button>
  </div>
</div>

<div class="container" id="mainApp">
  <h1>FaceMotion</h1>

  <div class="panel controls">
    <div class="file-btn">
      <button id="selectBtn">SELECT IMAGE</button>
      <input type="file" id="fileInput" accept="image/*">
    </div>
    
    <button id="analyzeBtn" disabled>ANALYZE EMOTIONS</button>
  </div>

  <div class="panel">
    <canvas id="canvas"></canvas>
  </div>

  <div class="panel" id="status">
    [SYSTEM] Status: Models Loaded. Ready For Input Image...
  </div>

  <div class="footer">Developed by Rakibuzzaman</div>
</div>

<script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

<script>
const loadModelsBtn = document.getElementById('loadModelsBtn');
const modalOverlay = document.getElementById('modalOverlay');
const mainApp = document.getElementById('mainApp');
const spinner = document.getElementById('spinner');
const fileInput = document.getElementById('fileInput');
const analyzeBtn = document.getElementById('analyzeBtn');
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');
const statusDiv = document.getElementById('status');

let loadedImg;

const explain = {
  happy: "Positive joy, satisfaction and confidence",
  sad: "Low mood, emotional pain or reflection",
  angry: "High arousal, frustration or dominance",
  fearful: "Stress, anxiety or alert response",
  disgusted: "Rejection or strong dislike",
  surprised: "Sudden stimulus or shock",
  neutral: "Emotionally balanced or controlled"
};

function log(t) { statusDiv.innerText = "[SYSTEM] " + t; }

// 1. Model Loading via Modal
loadModelsBtn.onclick = async () => {
  loadModelsBtn.style.display = 'none';
  spinner.style.display = 'block';
  log("Initializing Neural Networks...");

  try {
    const MODEL_URL = 'https://justadudewhohacks.github.io/face-api.js/models/';
    await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
    await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
    
    // On Success
    modalOverlay.style.display = 'none';
    mainApp.classList.add('active');
    log("Ready. Please Select An Image To Begin...");
  } catch (err) {
    alert("Error Loading Models. Please Check Your Internet Connection.");
    loadModelsBtn.style.display = 'block';
    spinner.style.display = 'none';
  }
};

// 2. Image Selection
fileInput.onchange = async (e) => {
  if(!e.target.files[0]) return;
  
  log("Processing image...");
  loadedImg = await faceapi.bufferToImage(e.target.files[0]);
  
  // Canvas Setup
  canvas.width = loadedImg.width;
  canvas.height = loadedImg.height;
  ctx.clearRect(0, 0, canvas.width, canvas.height);
  ctx.drawImage(loadedImg, 0, 0);
  
  analyzeBtn.disabled = false;
  log("Image Loaded. Press 'Analyze' To Scan Emotions.");
};

// 3. Emotion Analysis
analyzeBtn.onclick = async () => {
  log("Scanning Facial Emotional Patterns...");
  analyzeBtn.disabled = true;

  const detections = await faceapi
    .detectAllFaces(loadedImg, new faceapi.TinyFaceDetectorOptions())
    .withFaceExpressions();

  // Redraw image to clear previous boxes
  ctx.drawImage(loadedImg, 0, 0);
  faceapi.draw.drawDetections(canvas, detections);

  if (!detections.length) {
    log("No facial patterns detected. Try another image.");
    analyzeBtn.disabled = false;
    return;
  }

  let report = `Total Faces Detected: ${detections.length}\n\n`;

  detections.forEach((det, i) => {
    const exp = det.expressions;
    const sorted = Object.entries(exp).sort((a, b) => b[1] - a[1]);
    const [main, sec] = sorted;

    const intensity = main[1] > 0.7 ? "High" : main[1] > 0.4 ? "Medium" : "Low";
    
    report += `[FACE #${i+1}]\n`;
    report += `Primary: ${main[0].toUpperCase()} (${(main[1]*100).toFixed(1)}%)\n`;
    report += `Secondary: ${sec[0]} (${(sec[1]*100).toFixed(1)}%)\n`;
    report += `Intensity: ${intensity}\n`;
    report += `Insight: ${explain[main[0]]}\n\n`;
  });

  log(report);
  analyzeBtn.disabled = false;
};
</script>
</body>
</html>
